# YouTube Competitor Analysis Collector

This project contains a Celery-based worker for collecting and resolving YouTube channel information for competitor analysis.

## Running the Collector

### 1. Set Environment Variables

You need to provide your YouTube Data API keys. Create a `.env` file in the root of the project:

```
YT_API_KEYS=your_api_key_1,your_api_key_2
```

The `docker-compose.yml` file will automatically pick up this file.

### 2. Run with Docker Compose

To build and run the collector worker and its Redis dependency, use:

```bash
docker-compose up --build
```

You should see the Celery worker start up and log that it's ready to receive tasks.

### 3. Enqueueing Tasks

You can enqueue tasks using a Python script or an interactive shell (like `ipython` or `python -i`).

**Example: `enqueue_tasks.py`**

```python
from celery import Celery
from collector.config import settings

# This assumes your script is run from a context where it can import the collector modules.
# For a quick test, you can run this from the project root using `PYTHONPATH=. python enqueue_tasks.py`

celery_app = Celery(
    "competitor_analysis_collector",
    broker=settings.broker_url,
)

def start_analysis(analysis_id: int, owner_id: int, channels: list[str]):
    """
    This is a simplified example of how the main backend would
    start a new analysis run. It enqueues the initial jobs.
    """
    print(f"Starting analysis {analysis_id} for owner {owner_id}")

    # In a real application, the orchestrator would be called here.
    # For simplicity, we directly enqueue the tasks.

    # A real run_id would be generated by the state manager/DB. We'll use a placeholder.
    run_id = 1
    job_id_start = 1

    for i, channel_input in enumerate(channels):
        job_id = job_id_start + i
        print(f"Enqueuing job {job_id} for channel '{channel_input}'")
        celery_app.send_task(
            'collector.tasks.process_channel_job',
            args=[job_id, run_id],
            kwargs={'input_channel': channel_input, 'owner_id': owner_id}
        )

    # After some time, you might want to finalize the run.
    # This is usually triggered by a monitoring process.
    print(f"Enqueuing finalization for run {run_id}")
    celery_app.send_task('collector.tasks.finalize_run_task', args=[run_id])


if __name__ == "__main__":
    test_channels = [
        "https://www.youtube.com/channel/UC-lHJZR3Gqxm24_Vd_AJ5Yw", # Google
        "https://www.youtube.com/@MrBeast",
        "@nonexistenthandle12345",
        "some random text"
    ]
    start_analysis(analysis_id=101, owner_id=1, channels=test_channels)

```

To run this example, you'll need to have `celery` and `pydantic` installed in your local environment (`pip install celery pydantic`). Then, from the project root:

```bash
PYTHONPATH=. python enqueue_tasks.py
```

You should see the logs in the `collector_worker` container showing that the jobs are being processed.
